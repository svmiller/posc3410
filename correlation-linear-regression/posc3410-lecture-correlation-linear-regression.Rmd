---
title: "Correlation and Linear Regression"
subtitle: POSC 3410  -- Quantitative Methods in Political Science
author: Steven V. Miller
institute: Department of Political Science
titlegraphic: /Dropbox/teaching/clemson-academic.png
date: 
fontsize: 10pt
output:
 beamer_presentation:
    template: ~/Dropbox/miscelanea/svm-r-markdown-templates/svm-latex-beamer.tex
    latex_engine: xelatex
    dev: cairo_pdf
    fig_caption: false
    slide_level: 3
make149: true
mainfont: "Open Sans"
titlefont: "Titillium Web"
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
---

```{r setup, include=FALSE, cache=F, message=F, warning=F, results="hide"}
knitr::opts_chunk$set(cache=TRUE, warning=F, message=F)
knitr::opts_chunk$set(fig.path='figs/',  fig.width=14, fig.height=8.5)
knitr::opts_chunk$set(cache.path='cache/')

knitr::opts_chunk$set(
                  fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      }
                  )
```

```{r opts, cache=FALSE, eval=TRUE, echo=FALSE}

library(tidyverse)
library(stevemisc)
library(stevedata)

usa_states %>% select(stateabb, statename) %>% rename(state = statename) %>%
  left_join(election_turnout, .) -> election_turnout

```

# Introduction
### Goal for Today

*Use correlation and linear regression to describe the relationship between two interval-level variables.*

### Building Toward Normal Political Science

Everything we have done is building toward normal quantitative research.

- We have concepts of interest, operationalized to variables.
- We observe central tendencies and variation in our variables.
- We believe there is cause and effect.
	- Though, importantly, we need to make controlled comparisons.
- We make inference about our claim of cause and effect using the logic of random sampling.

If our sample statistic is more than 1.96 standard errors from a proposed population parameter, we have a lot of confidence (95%) rejecting the proposed population parameter.

### What We Will Be Doing Today

We'll go over the following two topics.

1. **Correlation analysis**
2. **Regression analysis**

### R Packages We'll Be Using

```r
library(tidyverse) # for all things workflow
library(stevemisc) # for various formatting things
library(stevedata) # for my toy data, including election_turnout
```

# Correlation
### Correlation

*Question*: does a state's voter turnout vary by the state's level of education?

- Education: % of state with high school diploma. (CPS estimates for 2015)
- Turnout: voter turnout for highest office (i.e. president) in 2016 general election.

We get a preliminary judgment using a **scatterplot**.

- But first: let's look at our data a bit.

### Students Always Ask These Questions...

Least-educated states in the U.S.

```{r}
election_turnout %>% select(state, perhsed) %>% 
  top_n(-5, perhsed) %>% arrange(perhsed)
```

### Be Mindful of Your Education Indicator...

```{r}
election_turnout %>% select(state, percoled) %>% 
  top_n(-5, percoled) %>% arrange(percoled)
```

### What About the Most Educated?

```{r}
election_turnout %>% select(state, perhsed) %>% 
  top_n(5, perhsed) %>% arrange(-perhsed)
```

### Again, College is Different...

```{r}
election_turnout %>% select(state, percoled) %>% 
  top_n(5, percoled) %>% arrange(-percoled)
```

### On Voter Turnout in 2016...

```{r}
election_turnout %>% select(state, turnoutho) %>% 
  top_n(5, turnoutho) %>% arrange(-turnoutho)
```

### Lowest Turnout States

```{r}
election_turnout %>% select(state, turnoutho) %>% 
  top_n(-5, turnoutho) %>% arrange(turnoutho)
```

### On South Carolina

If you're curious about South Carolina:

- 12th lowest in % of state with a college diploma (25.8%).
- 14th lowest in % of state with at least a HS diploma (85.6%).
- 12th lowest in voter turnout in 2016 (56.7%)

###

```{r scatter1, echo=F, eval=T, warning=F}

ggplot(election_turnout, aes(perhsed, turnoutho)) + 
  geom_point(size=I(2)) + theme_steve_web() +
  xlim(80,95) +
  scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
  labs(title = "A Scatterplot of State-Level Education and Voter Turnout in the 2016 General Election",
       subtitle = "The data are scattered in a formal consistent/positive way. Hawaii was always going to be a clear outlier.",
       x = "Percentage of Residents 25-years-and-older with at Least a High School Diploma",
       y = "Percentage Turnout of VEP for Highest Office",
       caption = "Data: Elections Project, U.S. Census Bureau. Assembled in stevedata package available on Github (svmiller/stevedata).")

```


### Correlation

This relationship looks easy enough: positive.

- The relationship is not perfect, but it looks fairly "strong".

How strong? **Pearson's correlation coefficient** (or **Pearson's *r***) will tell us.

### Pearson's *r*

$$
    \Sigma\frac{(\frac{x_i - \overline{x}}{s_x})(\frac{y_i - \overline{y}}{s_y})}{n - 1}
$$

...where:

- $x_i$, $y_i$ = individual observations of *x* or *y*, respectively.
- $\overline{x}$, $\overline{y}$ = sample means of *x* and *y*, respectively.
- $s_x$, $s_y$ = sample standard deviations of *x* and *y*, respectively.
- *n* = number of observations in the sample.

### Properties of Pearsons *r*

1. Pearson's *r* is symmetrical.
2. Pearson's *r* is bound between -1 and 1.
3. Pearson's *r* is standardized.

###

```{r scatterz, echo=F, eval=T, warning=F, error=F, message=F}
election_turnout %>%
  mutate(z_perhsed = (perhsed - mean(perhsed))/sd(perhsed),
         z_turnoutho = (turnoutho - mean(turnoutho))/sd(turnoutho)) %>%
  ggplot(.,aes(z_perhsed, z_turnoutho)) +
  geom_point(size=I(2)) + theme_steve_web() +
  # xlim(-2.2,2) + ylim(-3.3,3.3) +
  geom_vline(xintercept=0, linetype="dashed") +
  geom_hline(yintercept=0, linetype="dashed") +
  scale_y_continuous(breaks = c(-3,-2,-1,0,1,2,3), limits=c(-3.1,3)) +
  geom_text(aes(label=ifelse(z_perhsed > 0 & z_turnoutho < 0 ,as.character(stateabb),'')),hjust=-.5,vjust=0) +
  geom_text(aes(label=ifelse(z_perhsed < 0 & z_turnoutho > 0 ,as.character(stateabb),'')),hjust=-.5,vjust=0) +
  labs(title = "A Scatterplot of State-Level Education and Voter Turnout in the 2016 General Election",
       subtitle = "Observations in the negative correlation quadrants are highlighted for emphasis.",
       x = "Percentage of Residents 25-years-and-older with at Least a High School Diploma (Standardized)",
       y = "Percentage Turnout of VEP for Highest Office (Standardized)",
       caption = "Data: Elections Project, U.S. Census Bureau. Assembled in stevedata package available on Github (svmiller/stevedata).")

```

<!-- ### Education and Turnout (Z Scores)

![Scatterplot: Education and Turnout (Standardized)](fig82.pdf) -->

### Education and Turnout (Z Scores)

- Cases in upper-right quadrant are above the mean in both *x* and *y*.
- Cases in lower-left quadrant are below the mean in both *x* and *y*.
- Upper-left and lower-right quadrants are negative-correlation quadrants.

All told, our Pearson's *r* is 26.41369/50, or .52.

- We would informally call this a fairly strong positive relationship.

### ...or in R

\scriptsize
```{r orinr}

election_turnout %>%
  mutate(z_perhsed = (perhsed - mean(perhsed))/sd(perhsed),
         z_turnoutho = (turnoutho - mean(turnoutho))/sd(turnoutho)) -> election_turnout

with(election_turnout, sum(z_perhsed*z_turnoutho)/(length(state)-1))

with(election_turnout, cor(perhsed,turnoutho))

```
\normalsize

### If You're Curious about the Hawaii Outlier...

```{r hawaii}

election_turnout %>%
  filter(state != "Hawaii") %>%
  summarize(cor = cor(perhsed, turnoutho))


```

# Linear Regression
## Demystifying Regression
### Linear Regression

Correlation has a lot of nice properties.

- It's another "first step" analytical tool.
- Useful for detecting **multicollinearity**.
	- This is when two independent variables correlate so highly that no partial effect for either can be summarized.

However, it's neutral on what is *x* and what is *y*.

- It won't communicate cause and effect.

Fortunately, regression does that for us.

### Demystifying Regression

Does this look familiar?

$$
y = mx + b
$$

### Demystifying Regression

That was the slope-intercept equation.

- *b* is the intercept: the observed *y* when *x* = 0.
- *m* is the familiar "rise over run", measuring the amount of change in *y* for a unit change in *x*.

### Demystifying Regression

The slope-intercept equation is, in essence, the representation of a regression line.

- However, statisticians prefer a different rendering of the same concept measuring linear change.

$$
y = a + b(x)
$$

The *b* is the **regression coefficient** that communicates the change in *y* for each unit change in *x*.

## A Simple Example
### A Simple Example

Suppose I want to explain your test score (*y*) by reference to how many hours you studied for it (*x*).

| *Hours (x)* | *Score (y)* |
|:------------|:-----------:|
| 0 | 55 |
| 1 | 61 |
| 2 | 67 |
| 3 | 73 |
| 4 | 79 | 
| 5 | 85 | 
| 6 | 91 |
| 7 | 97 |

Table: Hours Spent Studying and Exam Score

<!-- ### 


![Hours Spent Studying and Test Score](tab81.pdf) -->

### A Simple Example

In this eight-student class, the cherub who studied 0 hours got a 55.

- The cherub who studied 1 hour got a 61.
- The cherub who studied 2 hours got a 67.
- ...and so on...

Each hour studied corresponds with a six-unit change in test score. Alternatively:

$$
y = a + b(x) = \textrm{Test Score} = 55 + 6(x)
$$

Notice that our *y*-intercept is meaningful.

### A Slightly Less Simple Example

However, real data are never that simple. Let's complicate it a bit.

\footnotesize

| *Hours (x)* | *Score (y)* | *Estimated Score ($\hat{y}$)* |
|:------------|:-----------:|:---------------------:|
| 0 | 53 | 55 |
| 0 | 57 | |
| 1 | 59 | 61 |
| 1 | 63 | |
| 2 | 65 | 67 |
| 2 | 69 | |
| 3 | 71 | 73 |
| 3 | 75 | |
| 4 | 77 | 79 |
| 4 | 81 | |
| 5 | 83 | 85 |
| 5 | 87 | |
| 6 | 89 | 91 |
| 6 | 93 | |
| 7 | 95 | 97 |
| 7 | 99 | |

Table: Hours Spent Studying, Exam Score, and Estimated Score 

\normalsize

<!-- ###

![Hours Spent Studying and Test Score](tab82.pdf)
-->

### A Slightly Less Simple Example

Complicating it a bit doesn't change the regression line.

- Notice that regression averages over differences.
- An additional hour studied, *on average*, corresponds with a six-unit increase in the exam score.
- We have observed data points (*y*) and our estimates ($\hat{y}$, or *y*-hat).

## Getting a Regression Coefficient
### Our Full Regression Line

Thus, we get this form of the regression line.

$$
\hat{y} = \hat{a} + \hat{b}(x) + e
$$

...where:

- $\hat{y}$, $\hat{a}$ and $\hat{b}$ are estimates of *y*, *a*, and *b* over the data.
- *e* is the error term.
	- It contains random sampling error, prediction error, and predictors not included in the model.

### Getting a Regression Coefficient

How do we get a regression coefficient for more complicated data?

- Start with the **prediction error**, formally: $y_i - \hat{y}$.
- Square them. In other words: $(y_i - \hat{y})^2$
	- If you didn't, the sum of prediction errors would equal zero.

The regression coefficient that emerges minimizes the sum of squared differences ($(y_i - \hat{y})^2$).

- Put another way: "ordinary least squares" (OLS) regression.

The next figure offers a representation of this for our state education and turnout example.

<!-- ### Regression: Education and Turnout

![Scatterplot: Education and Turnout (A Regression)](fig83.pdf) -->

### 

```{r scatterline, echo=F, eval=T, warning=F}

ggplot(election_turnout, aes(perhsed, turnoutho)) + 
  geom_point(size=I(2)) + theme_steve_web() +
  xlim(80,95) +
  scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
  geom_smooth(method=lm, se=FALSE) +
  labs(title = "Education and Turnout in the 2016 General Election",
       x = "Percentage of Residents 25-years-and-older with at Least a High School Diploma",
       y = "Percentage Turnout of VEP for Highest Office",
       subtitle = "The line that minimizes the sum of squared prediction errors is drawn through these points.")

```

### Standard Error of Regression Coefficient

Each parameter in the regression model comes with a "standard error."

- These estimate how precisely the model estimates the coefficient's unknown value.

This has a convoluted estimation procedure.

- Namely: you need the diagonal of the square root of the variance-covariance matrix.
- This requires matrix algebra, and you probably hate me enough. :P

It's standard output in a regression formula object in R, though.

### If You're Curious...

\footnotesize

```{r basiclm}
summary(M1 <- lm(turnoutho ~ perhsed, data=election_turnout))
```

\normalsize

### If You're Curious...

\scriptsize

```{r extractses}
X <- model.matrix(M1) # Intercept + perhsed

# Residual sum of squares
sigma2 <- sum((election_turnout$turnoutho - fitted(M1))^2) / (nrow(X) - ncol(X))

sqrt(sigma2) # residual standard error
sqrt(diag(solve(crossprod(X))) * sigma2) 
```

\normalsize

### 

```{r scatterlinese, echo=F, eval=T, warning=F}

ggplot(election_turnout, aes(perhsed, turnoutho)) + 
  geom_point(size=I(2)) + theme_steve_web() +
  xlim(80,95) +
  scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
  geom_smooth(method=lm, se=TRUE) +
    labs(title = "Education and Turnout in the 2016 General Election",
       x = "Percentage of Residents 25-years-and-older with at Least a High School Diploma",
       y = "Percentage Turnout of VEP for Highest Office",
       subtitle = "The line that minimizes the sum of squared prediction errors is drawn through these points.")

```

### Regression: Education and Turnout

This would be our regression line:

$$
\hat{y} = -32.30 + 1.05(x)
$$

How to interpret this:

- The state in which no one graduated from high school would have a voter turnout of -32.30%.
	- *Center your variables, people. Seriously...*
- Each unit increase in the percentage of the state's citizens having a high school diploma corresponds with an estimated 1.05% increase in voter turnout.

## Inference in Regression
### Inference in Regression

What do we say about that *b*-hat ($\hat{b}$ = 1.05?)

- If we took another "sample", would we observe something drastically different?
- How would we know?

### Inference in Regression

You've done this before. Remember our last set of lectures? And Z scores?

$$
Z = \frac{\overline{x} - \mu}{s.e.}
$$

### Inference in Regression

We do the same thing, but with a Student's *t*-distribution.

$$
t = \frac{\hat{b} - \beta}{s.e.}
$$

$\hat{b}$ is our regression coefficient. What is out $\beta$?

### Inference in Regression

$\beta$ is actually zero!

- We are testing whether our regression coefficient is an artifact of the "sampling process".
- We're testing a competing hypothesis that there is no relationship between *x* and *y*.

### Inference in Regression

This makes things a lot simpler.

$$
t = \frac{\hat{b}}{s.e.}
$$

### Inference in Regression

In our state education and turnout example, this turns out nicely.

$$
t = \frac{1.05}{.24} = 4.35
$$

Our regression coefficient is more than four standard errors from zero .

- The probability of observing it if $\beta$ were really zero is .000067.
- We judge our regression coefficient to be statistically significant.

# Conclusion
### Conclusion

Hopefully, this lecture demystified regression.

- It builds on everything discussed to this point.
- The same process of inference from sample to population is used.
- Really nothing to it but to do it, I 'spose.

Weâ€™re going to add a fair bit on top of this next.

- If you understand this, everything else to follow is basically window dressing.

